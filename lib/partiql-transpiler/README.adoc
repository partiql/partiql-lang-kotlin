= PartiQL Transpiler
:toc:

The PartiQL Transpiler is a compiler framework for the PartiQL SQL dialect.
It is considered experimental and is under active development.

== Terms

* *SQL* — Specifically the SQL-99 Data Query Language specification, colloquially select-from-where
* *Dialect* — An implementation of the SQL language specification
* *Target* — A representation of some computation
* *Catalog* — Schemas, tables, types, functions, and operators available in a target

== Usage

The transpiler leverages PartiQL's plugin system and planner to produce a resolved and typed logical query plan.
This plan is passed to a _target_ implementation to be transformed to the domain specific output.

NOTE: Much of the transpiler involves manipulating both the AST and Plan which are PartiQL intermediate representations.
This xref:https://github.com/partiql/partiql-lang-kotlin/blob/main/partiql-ast/README.adoc[PartiQL AST README] has tips on working with these structures.

.Creating the Transpiler
[source,kotlin]
----
// PartiQL's plugin system is how you provide tables and schemas to the planner (à la Trino).
val plugin = MyPlugin()

// Instantiate the transpiler once. It can be re-used!
val transpiler = PartiQLTranspiler(listOf(plugin))
----

Suppose you have some table

[source,sql]
----
CREATE TABLE orders (
    order_id   STRING PRIMARY KEY, -- PartiQL STRING type
    ordered_at DATETIME NOT NULL   -- PartiQL DATETIME type
);
----

How do you express this query for a different SQL engine like Trino?

[source,kotlin]
----
// Get all orders in the last 30 days
val query = """
    SELECT order_id FROM orders
    WHERE ordered_at > date_diff(day, -30, UTCNOW())
"""

// TrinoTarget holds the translation rules from a PartiQL plan to Trino SQL
val target = TrinoTarget()

// Planner session, assuming your table `orders` exists in the "default" catalog
val session = PartiQLPlanner.Session(
    queryId = "readme_query_id",
    userId = "readme_user_id",
    currentCatalog = "default",
)

// Invoke the transpiler
val result = transpiler.transpile(query, target, session)

println(result.value)
// Output:
//   SELECT orders.order_id AS order_id FROM orders AS orders
//   WHERE orders.ordered_at > date_add('day', -30, at_timezone(current_timestamp, 'UTC'))
----

=== REPL

There's a basic REPL which can be useful for rapid experimentation. You can also attach a debugger to the REPL.

NOTE: The partiql-byob package contains the TPC-DS schemas in `catalogs/`.

.Debugging
[source,bash]
----
# Install
./gradlew :lib:partiql-byob:install

# Execute
> $ ./lib/partiql-byob/build/install/byob/bin/byob --help

Usage: transpile [-hV] [--catalog=<catalog>] -d=DIR

The PartiQL Transpiler Debug REPL
This REPL is used for debugging the transpiler
      --catalog=<catalog>   Catalog, use `default` .. by default
                              Default: default
  -d=DIR                    Database root directory
  -h, --help                Show this help message and exit.
  -V, --version             Print version information and exit.


# attach a debugger by prepending, then you can connect to localhost:505 in intellij
JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=localhost:5050 /path/to/bin/byob
----


== Overview

The PartiQL Transpiler is a framework to plug different compilation backends.
Perhaps this project should be renamed to BYOB (bring your own backend).
For now, we only provide SQL source-to-source compilation (hence "transpile"), but you could conceive of several non-SQL targets such as:

* xref:https://substrait.io/[Substrait]
* xref:https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html[Spark Dataset Closure]
* xref:https://beam.apache.org/documentation/basics/[Apache Beam Transform]
* xref:https://calcite.apache.org/docs/algebra.html[Calcite relational algebra]

=== Producing SQL

For now, the transpiler provides two simple SQL text targets.
Each dialect is _quite_ similar (hence dialect) so much of the base translation from PartiQL's logical plan to an SQL AST is captured by `org.partiql.transpiler.sql.SqlTransform`.

This applies a transformation of relational algebra to an SQL AST just like Calcite's xref:https://github.com/apache/calcite/blob/main/core/src/main/java/org/apache/calcite/rel/rel2sql/RelToSqlConverter.java[RelToSqlConverter]; however, this is currently more limited than Calcite's.

Much of the differences between dialects comes down to scalar functions, but it's often the case that each dialect has functions with similar functionality albeit different names.
This is shown in the earlier `UTCNOW()` example.

=== Common Interfaces

The most useful interfaces to implement for an SQL target are

* `TpTarget<T>` — Base transpiler target interface
* `SqlTarget` — Base `TpTarget<String>` implementation for an SQL dialect target
* `SqlCalls` — Ruleset for rewriting scalar calls
* `SqlTransform` — Ruleset for RelToSql conversion

== Development

Let's work through an example of developing our own SQL target using SQLite as the target.
How might we transpile?

[source,sql]
----
SELECT CAST(a AS STRING) FROM T
----

With basic familiarity of SQLite, we know that `STRING` is not a valid type name, and we should replace it with `TEXT`.
How do we express this in a transpilation target?

=== Tutorial

.Extend SqlTarget
[source,kotlin]
----
public object SQLiteTarget : SqlTarget() {

    override val target: String = "SQLite"

    // Using SQLite3
    override val version: String = "3"

    // Override the default call ruleset with the SQLiteCalls ruleset
    override fun getCalls(onProblem: Problemhandler): SqlCalls = SQLiteCalls()

    // No need to rewrite the plan, return as is
    override fun rewrite(plan: PartiQLPlan, onProblem: ProblemCallback) = plan
}
----

NOTE: I'm conflicted on how to pass the problem handler to SqlCalls, so that's subject to change.

.Provide Scalar Function Ruleset
[source,kotlin]
----
@OptIn(PartiQLValueExperimental::class)
public class SQLiteCalls : SqlCalls() {

    /**
    * SqlCalls has many open functions which you can extend to override for edge cases.
    */
    override fun rewriteCast(type: PartiQLValueType, args: SqlArgs): Expr = Ast.create {
        if (type == PartiQLValueType.STRING) {
            // do something special for `CAST(.. AS STRING)`
            Ast.create { exprCast(args[0].expr, typeCustom("TEXT")) }
        } else {
            return super.rewriteCast(type, args)
        }
    }
}
----

This is reasonable, but what about replacing all occurrences of STRING with TEXT?
It would be a cumbersome to track down all the places a type might be used (like this `IS` special form is another).

We can actually _also_ extend how SQL is rendered to text via an extendable query printing framework.
See xref:https://github.com/partiql/partiql-lang-kotlin/pull/1183[Pull #1183].
You can provide the pretty-printer a _Dialect_ which contains base behavior for translating from an AST to a Block tree where the Block tree is a basic formatting structure.

Let's implement `SQLiteDialect` and wire it to our `SQLiteTarget`.

.Defining a Dialect
[source,kotlin]
----
public object SQLiteDialect : SqlDialect() {

    override fun visitTypeString(node: Type.String, head: SqlBlock) =
        SqlBlock.Link(head, SqlBlock.Text("TEXT"))
}
----

.Providing the Dialect
All this says is during the fold from an AST to Block tree, is to append the string "TEXT" to the tree.
We can use this dialect for our target by overriding the `dialect` field.

[source,kotlin]
----
public object SQLiteTarget : SqlTarget() {

    // ... same as before

    // hook up the pretty-printer rules
    override val dialect = SQLiteDialect
}
----

== Testing

The PartiQL Transpiler project has a basic testing framework.
Test cases _inputs_ are specified in the `test/resources/`
as _suites_ which are groups of related tests.
Each suite will have some catalog configuration and map of all tests.

=== Executing Tests

[source,shell]
----
# All tests
./gradlew :lib:partiql-transpiler:test

# For a single target
./gradlew :lib:partiql-transpiler:test --tests "org.partiql.transpiler.test.targets.redshift.*"
----

IMPORTANT: Each test should be uniquely identifiable by the suite name and the test case key.
This is because test inputs are separated from test targets and must be correlated.
The implication is that every target is responsible for implementing their own assertions to the shared corpus of inputs.

=== Test Input Format

Test cases are currently stored in `test/resources/cases` as Ion documents.
Here is an example:

[source,ion]
----
suite::{
  name: "transpiler_suite_00",    // Unique suite name
  session: {                      // Session configuration for the entire suite
    catalog: "default",           // Session catalog
    path: ["tpc_ds"],             // Session path (search_path / current directory)
    vars: {},                     // Arbitrary Map<String, String> session variables (CURRENT_USER, etc.)
  },
  tests: {
    '0000': {                     // Map<String, Test> — map keys should be unique for lookup
      statement: '''
        -- PartiQL input as Ion multiline string
        SELECT TRIM(LEADING FROM t.a)
        FROM T as t
      ''',
      schema: {                   // Expected query output schema notated as PartiQL Value Schema
        type: "bag",              // See Appendix I for details
        items: {
          type: "struct",
          fields: [
            {
              name: "a",
              type: "string",
            },
          ],
        },
      },
    },
  },
}
----

=== Test Target Format

Targets are responsible for defining their own expected outputs for each test case.
This means that a target could actually implement its assertions however it pleases. In our examples, we are compiling
PartiQL queries to SQL dialects, so we will assert on the output SQL.

Let's walk through the Trino example test cases. Target test assertions are stored in `test/resources/targets`. The
Trino target assertion format looks like this:

[source,ion]
----
target::{
  name: "trino",
  suite: "transpiler_suite_00",
  tests: {
    '0000': {
      statement: '''
        SELECT ltrim(t.a) FROM T as t
      ''',
    },
  },
}
----

We expect PartiQL's special form `TRIM(LEADING FROM t.a)` to map to Trino's xref:https://trino.io/docs/current/functions/string.html#ltrim[`ltrim(_string_)`].
The testing framework has a base class which will generate Junit tests for each input. The target is responsible for
mapping test cases to the expected values. Let's see how we would perform assertions in our Trino example.

See `test/kotlin/org/partiql/transpiler/test/targets/trino` for the full implementation.

[source,kotlin]
----
/**
 * We extend from the TranspilerTestFactory and need only implement `assert`.
 */
class TrinoTargetTestFactory : TranspilerTestFactory<String>(TrinoTarget) {

    // A place to lookup a test given its key
    private val suites: Map<String, TrinoTargetTestSuite>

    // Load all expected results
    init {
        val testDir = TrinoTargetTest::class.java.getResource("/targets/trino")!!.toURI().toPath()
        val testFiles = testDir.toFile().listFiles()!!
        suites = testFiles.associate {
            val text = it.readText()
            val ion = loadSingleElement(text)
            assert(ion is StructElement) { "Test suite file must be a single struct" }
            val suite = TrinoTargetTestSuite.load(ion as StructElement)
            suite.name to suite
        }
    }

    // The base TranspilerTestFactory calls this for every test. Perform your target specific assertions here!
    override fun assert(
        suiteKey: String,
        testKey: String,
        test: PlannerTest,
        result: PartiQLTranspiler.Result<String>,
    ) {
        val expected = lookup(suiteKey, testKey)
        val expectedNormalized = normalize(expected.statement)
        val actualNormalized = normalize(result.output.value)
        assertEquals(expectedNormalized, actualNormalized)
    }

    // Attempt to lookup this test, skipping if the Assumptions fail
    private fun lookup(suiteKey: String, testKey: String): TrinoTargetTest {
        val suite = suites[suiteKey]
        Assumptions.assumeTrue(suite != null)
        val test = suite!!.tests[testKey]
        Assumptions.assumeTrue(test != null)
        return test!!
    }

    /**
     * We're comparing string equality now.
     */
    private fun normalize(query: String): String = query.lines().joinToString(" ") { it.trim() }.trim()
}
----

It is advised to use `Assumptions.assume...` to skip a test rather than fail iff your target does not have a solution.
This will allow the build to succeed even if your target isn't able to fully satisfy the input cases. In the future,
this test suite may exist outside the transpiler's unit tests.

== Appendix

=== I. PartiQL Value Schema Language

Testing schemas are described using a modified version of the xref:https://docs.oracle.com/cd/E26161_02/html/GettingStartedGuide/avroschemas.html#avro-complexdatatypes[Avro JSON schema].
The changes are (1) it's Ion and (2) we use the PartiQL type names.

.Basic Type Schema Examples
[source,ion]
----
// type name atomic types
"int"

// type list for union types
[ "int", "null" ]

// Collection Type
{
  type: "bag",  // valid values "bag", "list", "sexp"
  items: <type>
}

// Struct Type
{
  type: "struct",
  fields: [
    {
      name: "foo",
      type: <type>
    },
    // ....
  ]
}
----

IMPORTANT: For now, we omit constraints such as open/closed structs.

=== II. PartiQL FS Plugin

The PartiQL FS Plugin builds a catalog from a directory tree. It is implemented here https://github.com/partiql/partiql-lang-kotlin/tree/transpile/partiql-planner/src/testFixtures/kotlin/org/partiql/planner/test/plugin.

NOTE: Directories are nested schemas; files represent table schema where the table name is the file name (without .ion).

It will eventually be merged with mockdb. The only difference is that is meoizes parsed schemas and uses PartiQL Value Schema
syntax.
